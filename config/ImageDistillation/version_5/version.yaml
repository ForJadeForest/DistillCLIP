model:
  class_path: DistillModel
  init_args:
    loss_control_para:
      loss_name: [ 'out_l1', 'out_cos', 'attention_probs_mse', 'hidden_rep_mse']
      loss_scale:
        out_l1: 1
        out_cos: 1
        attention_probs_mse: 1
        hidden_rep_mse: 1
      temperature: 1


trainer:
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      save_dir: '/data/pyz/result/Dis_CLIP'
      name: 'weight share-attention_probs_mse-hidden_rep_mse'
      project: 'ImageDistillation'
      log_model: false
      group: weight_share
      version: 'weight share-attention_probs_mse-hidden_rep_mse-baseline'
      tags: ['baseline', 'weight share', 'loss change']