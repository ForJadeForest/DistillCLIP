model:
  class_path: DistillModel
  init_args:
#    init_type: 'mid'
    teacher_name: ViT-B/32
    download_root: '/data/pyz/.cache'
    loss_control_para:
      loss_name: ['l1', 'cos']

trainer:
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      dir: '/data/pyz/result/Dis_CLIP'
      name : 'ImageEncoderBaseLine_l1_cos_16_all'
      project: 'ImageEncoderDistillation'
      version: null
      log_model: false
      sync_tensorboard: True

data:
  class_path: DistillationDataModule
  init_args:
    num_workers: 12
    dataset: '_dataset'
    train_batch_size: 1024
    kwargs:
      data_dir: '/data/pyz/data'
      dataset_name: 'ImageDataset'
      cache_dir: '/data/pyz/data/cache'
      train_dir: '/home/pyz/combine_dataset/'
      image_use: 'all'