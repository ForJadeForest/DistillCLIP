model:
  class_path: DistillModel
  init_args:
#    init_type: 'mid'
    download_root: '/data/pyz/.cache'
    loss_control_para:
      loss_name: ['l1', 'cos']
    student_encoder:
      class_path: model.image_encoder.ImageEncoder
      init_args:
        is_student: True
        tea_transformer_width: 768
        vit_paras:
          input_resolution: 224
          patch_size: 32
          width: 768
          layers: 4
          heads: 12
          output_dim: 512
        drop_out: 0
    lr: 5e-3
    warm_steps: 20
    total_steps: 280
    teacher_name: ViT-B/32



trainer:
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      dir: '/data/pyz/result/Dis_CLIP'
      name : 'ImageEncoder_no_dropout'
      project: 'ImageEncoderDistillation'
      version: null
      log_model: false
      sync_tensorboard: True
  max_epochs: 300

data:
  class_path: DistillationDataModule
  init_args:
    num_workers: 12
    dataset: '_dataset'
    train_batch_size: 1024
    kwargs:
      data_dir: '/data/pyz/data'
      dataset_name: 'ImageDataset'
      cache_dir: '/data/pyz/data/cache'
      train_image_dir: '/home/pyz/combine_dataset/'
      image_use: [ 'imagenet', 'coco' ]
      cache_path: '/data/pyz/.cache/CLIPDistill'
      overwrite: False