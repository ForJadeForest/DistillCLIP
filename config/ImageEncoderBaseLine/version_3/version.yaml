model:
  class_path: DistillModel
  init_args:
#    init_type: 'mid'
    teacher_name: ViT-B/32
    download_root: '/data/pyz/.cache'
    loss_control_para:
      loss_name: ['emb', 'attn_probs', 'hidden', 'kl']
      loss_scale:
        emb: 0.5
        attn_probs: 100
        hidden: 0.2
      percent:
        emb: 0.1
        attn_probs: 0.3
        hidden: 0.3
        kl: 0.3
      temperature: 1
    map_type: 'mid'



trainer:
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      dir: '/data/pyz/result/Dis_CLIP'
      name : 'ImageEncoderBaseLine_tiny_16_clip_grad'
      project: 'ImageEncoderDistillation'
      version: null
      log_model: false
      sync_tensorboard: True
  gradient_clip_val: 5