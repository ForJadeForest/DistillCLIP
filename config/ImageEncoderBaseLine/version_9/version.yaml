model:
  class_path: DistillModel
  init_args:
    teacher_name: ViT-B/32
    init_type: 'begin'
    download_root: '/data/pyz/.cache'
    loss_control_para:
      # 'l1' 'ce' 'kl' 'cos' 'emb' 'attn' 'attn_probs' 'hidden' 'last_attn' 'last_attn_probs' 'last_value_map':
      loss_name: ['l1', 'cos', 'last_attn', 'last_value_map']
      loss_scale:
        l1: 1
        cos: 1
        last_attn: 0.001
        last_value_map: 1
      need_reduce: False
    lr: 1.0e-4

trainer:
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      dir: '/data/pyz/result/Dis_CLIP'
      name : 'ImageEncoderBaseLine_last_attn_value_16_init_begin '
      project: 'ImageEncoderDistillation'
      version: null
      log_model: false
      sync_tensorboard: True
  precision: 16
  devices: [ 2 ]
  max_epochs: 200
#  strategy: 'ddp'