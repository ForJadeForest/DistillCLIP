model:
  class_path: DistillModel
  init_args:
    weight_decay: 1.0e-2
    student_encoder:
      class_path: model.weight_share_model.RepeatVisionTransformer
      init_args:
        img_size: 224
        patch_size: 32
        in_chans: 3
        out_dim: 512
        embed_dim: 768
        depth: 4
        num_heads: 12
        mlp_ratio: 4.0
        qkv_bias: True
        drop_rate: 0.1
        attn_drop_rate: 0.1
        drop_path_rate: 0.1
        use_cls_token: True
        repeated_times: 2
        use_transform: True
    teacher_name: ViT-B/32
    download_root: '/data/pyz/.cache'
    model_type: 'image'
    total_steps: 300
    loss_control_para:
      loss_name: [ 'l1', 'cos' ]




data:
  class_path: DistillationDataModule
  init_args:
    num_workers: 12
    dataset: '_dataset'
    train_batch_size: 1560
    kwargs:
      data_dir: '/data/pyz/data'
      dataset_name: 'ImageDataset'
      cache_dir: '/data/pyz/data/cache'
      train_image_dir: '/home/pyz/combine_dataset/'
      image_use: ['imagenet', 'coco']
      cache_path: '/data/pyz/.cache/CLIPDistill'
      overwrite: False





trainer:
  auto_select_gpus: True
  accumulate_grad_batches: 1
  accelerator: 'gpu'
  devices: [0, 1, 2, 3]
  max_epochs: 300
  strategy: 'ddp'
  log_every_n_steps: 50
  enable_progress_bar: True
  check_val_every_n_epoch: 1
  precision: 16

  callbacks:
    - class_path: LearningRateMonitor
    - class_path: EarlyStopping
      init_args:
        monitor: 'val/loss'
        patience: 15

    - class_path: ModelSummary
      init_args:
        max_depth: 2
    - class_path: ModelCheckpoint
      init_args:
        filename: '{epoch}-val_acc{hp_metric/stu_acc_top1:.2f}-loss{val/loss}'
        monitor: 'hp_metric/stu_acc_top1'
        save_last: True
        save_top_k: 4
        mode: 'max'
        auto_insert_metric_name: false
    - class_path: ModelCheckpoint
      init_args:
        filename: '{epoch}-val_acc{hp_metric/stu_acc_top1:.2f}-loss{val/loss}'
        monitor: 'val/loss'
        save_last: True
        save_top_k: 4
        mode: 'max'
        auto_insert_metric_name: false
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      dir: '/data/pyz/result/Dis_CLIP'
      name : 'WeightShareDistillation_test'
      project: 'WeightShareDistillation'
      version: null
      log_model: false
      sync_tensorboard: True

#optimizer:
#  class_path: AdamW
#  init_args:
#    lr: 1.0e-3
#    weight_decay: 0.0001
#    eps: 1.0e-8
#
#
#lr_scheduler:
#  class_path: get_cosine_schedule_with_warmup
#  init_args:
#    num_warmup_steps: 5
#    num_training_steps: 200