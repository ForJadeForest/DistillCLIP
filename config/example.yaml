model:
  class_path: Inception
  # Resnet's parameter
  init_args:
    in_channel: 3


data:
#  class_path: MNISTDataModule
  class_path: CIFARDataModule
  # MNISTDataModule
  init_args:
    data_dir: './data'
    batch_size: 128


trainer:
  profiler:
    class_path: pytorch_lightning.profiler.SimpleProfiler
    init_args:
      dirpath: './'
      filename: 'profile.log'
  auto_select_gpus: True # If enabled and gpus or devices is an integer, pick available gpus automatically.
  accumulate_grad_batches: 1
  accelerator: 'gpu'
  max_epochs: 5
  min_epochs: 3
  max_steps: -1 # Stop training after this number of steps. Disabled by default (-1). If max_steps = -1 and max_epochs = None, will default to max_epochs = 1000. To enable infinite training, set max_epochs to -1.
  devices: 1
  precision: 32
#  log
  log_every_n_steps: 10 # How often to log within steps. Default: 50
  logger:
    class_path: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    init_args:
      save_dir: 'res'
      name: 'resnet18_v2'
      version: null
  enable_progress_bar: True # show the progressbar
  check_val_every_n_epoch: 1 # valid after n epoch. If None, validation will be done solely based on the number of training batches, requiring val_check_interval to be an integer value.
  val_check_interval: 0.5 # check valid with n batch or n * total_batch_num when check_val_every_n_epoch is None
#  callbacks:
  callbacks:
    - class_path: LearningRateMonitor
    - class_path: ModelCheckpoint
      init_args:
        filename: '{epoch}-val_acc{valid/acc:.2f}-loss{valid/loss}'
        monitor: 'valid/acc'
        save_last: True
        save_top_k: 3
        mode: 'max'
        auto_insert_metric_name: False
    - class_path: EarlyStopping
      init_args:
        monitor: 'valid/loss'
    - class_path: ModelSummary
      init_args:
        max_depth: 2
#  auto_lr_find: True
#  auto_scale_batch_size: 'binsearch'
#  amp_backend: 'apex'  # or native
#  resume_from_checkpoint: '/load/my/checkpoint'
#  strategy: 'ddp'
#  Debug
#  limit_train_batches: 0.5
#  limit_val_batches: 0.2 # or int 3 means 3 batches
#  limit_test_batches:
#  fast_dev_run: True
#  limit_predict_batches
#  overfit_batches: 0.0 # Overfit a fracion of training/validation data (float) or a set number of batches (int). Default: 0.0.t



optimizer:
  class_path: Adam
  init_args:
    lr: 1.0e-5

lr_scheduler:
  class_path: CosineAnnealingLR
  init_args:
    T_max: 20