model:
  loss_control_para:
    loss_name: [ 'out_l1', 'out_cos' ]
    loss_scale:
      out_l1: 1
      out_cos: 1

    temperature: 1
    vit_kd_para:
      student_dims: 768
      teacher_dims: 768
      low_layers_num: 2
      high_layers_num: 1
    normalize: False
  warm_steps: 15
  total_steps: 2000

trainer:
  max_epochs: 2000
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      dir: '/data/pyz/result/dis_clip'
      name: 'ws-l1-cos-1000epoch'
      project: 'CLIPDistillation'
      log_model: false
      group: weight_share
      tags: [ 'weight share', 'loss change' ]
